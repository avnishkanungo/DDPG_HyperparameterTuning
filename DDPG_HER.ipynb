{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('FetchReach-v2')\n",
    "print(env.reset())\n",
    "achieved_g,desired_g,state,state_prime = unpackObs(env.reset())\n",
    "# achieved_g = normalize_to_minus_one_one(achieved_g)\n",
    "# desired_g = normalize_to_minus_one_one(desired_g)\n",
    "# state = normalize_to_minus_one_one(state)\n",
    "# state_prime = normalize_to_minus_one_one(state_prime)\n",
    "print(achieved_g,desired_g,state,state_prime)\n",
    "action = agent.select_action(state[:10])\n",
    "print(action)\n",
    "print(env.step(action))\n",
    "observation, reward, done,x ,info = env.step(action)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNet(state_size, action_size, hidden_size, max_action).to(device)\n",
    "with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state[:10]).to(device)\n",
    "            action = actor(input_state)\n",
    "            #detach and turn to numpy to use with np.random.choice()\n",
    "            action = action.detach().cpu().numpy()\n",
    "            #in DDPG add noise for exploration\n",
    "            action = (action + np.random.normal(0.,[0.10463545140160027, 0.009425990872921002, 0.21728112610138017, 0.20284884391940886], \n",
    "                       size=env.action_space.shape[0])).clip(env.action_space.low[0], env.action_space.high[0])\n",
    "            print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_minus_one_one(lst):\n",
    "    min_val = min(lst)\n",
    "    max_val = max(lst)\n",
    "    range_val = max_val - min_val\n",
    "    normalized = [((x - min_val) / range_val) * 2 - 1 for x in lst]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few frames of CartPole\n",
    "for i in range(100):\n",
    "    # do not use env.render() with jupyter notebook\n",
    "    # env.render() \n",
    "    action = env.action_space.sample()\n",
    "    print(env.step(action))\n",
    "    obs, reward, done, x,info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "        \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()[0]['achieved_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(env.step(action))\n",
    "obs, reward, done, x,info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def unpackObs(obs):\n",
    "    return  obs[0]['achieved_goal'], obs[0]['desired_goal'], np.concatenate((obs[0]['observation'], obs[0]['desired_goal'])),np.concatenate((obs[0]['observation'], obs[0]['achieved_goal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpackObs(env.reset())\n",
    "assert(len(unpackObs(env.reset())) == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpackObs(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(5, \"apple\"), (2, \"banana\"), (9, \"cherry\")]\n",
    "x = []\n",
    "for i in range(1,len(pairs)):\n",
    "    x.append(pairs[i-1])\n",
    "    # x.append(pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_PREFIX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SB3 Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from stable_baselines3 import PPO, DDPG, HerReplayBuffer, TD3, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"FetchReach-v2\"\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "goal_selection_strategy = \"future\"\n",
    "# model = DDPG('MultiInputPolicy', \n",
    "#             env, \n",
    "#             action_noise=action_noise,\n",
    "#             replay_buffer_class = HerReplayBuffer, \n",
    "#             replay_buffer_kwargs=dict(\n",
    "#                 n_sampled_goal=4,\n",
    "#                 goal_selection_strategy=goal_selection_strategy,\n",
    "#             ), \n",
    "#             verbose=1)\n",
    "model = DDPG(\n",
    "        \"MultiInputPolicy\",\n",
    "        env,\n",
    "        action_noise= action_noise,\n",
    "        verbose=0,\n",
    "        gamma= 0.9741273742139606,\n",
    "        tau=0.022108947086237923,\n",
    "        batch_size=166,\n",
    "        buffer_size=85344,\n",
    "        learning_rate= 0.0009678496054008291,\n",
    "        policy_kwargs= dict(\n",
    "            activation_fn = nn.ReLU,\n",
    "        ),\n",
    "        learning_starts=100,\n",
    "        replay_buffer_class=HerReplayBuffer,\n",
    "        replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=\"future\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# {'gamma': 0.9741273742139606, 'tau': 0.022108947086237923, 'batch_size': 166, 'buffer_size': 85344, 'learning_rate': 0.0009678496054008291, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"FetchReach-v2\"\n",
    "# Env used only for evaluation\n",
    "eval_envs = make_vec_env(env_id, n_envs=10)\n",
    "# 4000 training timesteps\n",
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Mean episode reward: -49.18 +/- 3.45\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_envs, n_eval_episodes=1000, deterministic=True)\n",
    "\n",
    "print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "environment_name = \"FetchReach-v2\"\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "goal_selection_strategy = \"future\"\n",
    "model = DDPG('MultiInputPolicy', \n",
    "            env, \n",
    "            action_noise=action_noise,\n",
    "            replay_buffer_class = HerReplayBuffer, \n",
    "            replay_buffer_kwargs=dict(\n",
    "                n_sampled_goal=4,\n",
    "                goal_selection_strategy=goal_selection_strategy,\n",
    "            ), \n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.compute_reward` for environment variables or `env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 80       |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 200      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.727    |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.3      |\n",
      "|    critic_loss     | 0.172    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 499      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85     |\n",
      "|    critic_loss     | 0.129    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.53     |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 899      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = CheckpointCallback(save_freq=20, save_path='/teamspace/studios/this_studio/optimized_logs/', name_prefix='ddpg_model')\n",
    "model.learn(total_timesteps=1000, callback=checkpoint_callback) #learn \n",
    "ddpg_path = '/teamspace/studios/this_studio/FetchReach_model_ddpg_optimized'\n",
    "model.save(ddpg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimzation uisng SB3 on TD3 with HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (24.0)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 greenlet-3.0.3 optuna-3.6.1 sqlalchemy-2.0.30\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 1000  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 5\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "ENV_ID = \"FetchReach-v2\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MultiInputPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "    # \"buffer_size\":1000000,\n",
    "    # # \"action_noise\":action_noise,\n",
    "    # \"replay_buffer_class\" : HerReplayBuffer,\n",
    "    # \"replay_buffer_kwargs\":{\n",
    "    #     \"n_sampled_goal\":4,\n",
    "    #     \"goal_selection_strategy\":\"future\",\n",
    "    # }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for A2C hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    # Discount factor between 0.9 and 0.9999\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    # 8, 16, 32, ... 1024\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO:\n",
    "    # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`\n",
    "    # - define the network architecture search space [\"tiny\", \"small\"] -> `suggest_categorical`\n",
    "    # - define the activation function search space [\"tanh\", \"relu\"]\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\",\"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]}\n",
    "        if net_arch == \"tiny\"\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sample_ddpg_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DDPG hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    # Discount factor between 0.9 and 0.9999\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    tau = trial.suggest_float(\"tau\", 0, 1, log=False)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 64,256,log=False )\n",
    "    buffer_size = trial.suggest_int(\"buffer_size\", 10000, 1000000, log=True)\n",
    "    # max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    # 8, 16, 32, ... 1024\n",
    "    # n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO:\n",
    "    # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`\n",
    "    # - define the network architecture search space [\"tiny\", \"small\"] -> `suggest_categorical`\n",
    "    # - define the activation function search space [\"tanh\", \"relu\"]\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\",\"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "    # n_sampled_goal = trial.suggest_int(\"n_sampled_goal\", 1, 10, log=False)\n",
    "    # goal_selection_strategy = trial.suggest_categorical(\"goal_selection_strategy\", [\"future\",\"final\",\"episode\"])\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    # trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64,64], \"vf\": [64,64]}\n",
    "        if net_arch == \"tiny\"\n",
    "        else {\"pi\": [400, 300], \"vf\": [400, 300]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    # goal_selection_strategy = {\"future\": \"future\", \"final\": \"final\", \"episode\": \"episode\"}[goal_selection_strategy]\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        # \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"tau\":tau,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"buffer_size\":buffer_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        # \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            # \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = False,\n",
    "        verbose: int = 0,\n",
    "        best_model_save_path: str = \"/teamspace/studios/this_studio/A2C_Mode\",\n",
    "\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "            best_model_save_path = best_model_save_path,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "    params = sample_ddpg_params(trial)\n",
    "    \n",
    "    env = gym.make(environment_name)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "\n",
    "    # kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    ### YOUR CODE HERE\n",
    "    #: \n",
    "    # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`\n",
    "    # 2. Create the evaluation envs\n",
    "    # 3. Create the `TrialEvalCallback`\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    print(sample_ddpg_params(trial))\n",
    "    #kwargs.update(sample_a2c_params(trial))\n",
    "    # kwargs.update(sample_ddpg_params(trial))\n",
    "\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    \n",
    "    # Create the RL model\n",
    "    #model = A2C(**kwargs)\n",
    "    model = DDPG(\n",
    "        \"MultiInputPolicy\",\n",
    "        env,\n",
    "        action_noise=action_noise,\n",
    "        verbose=0,\n",
    "        **params,\n",
    "        learning_starts=100,\n",
    "        replay_buffer_class=HerReplayBuffer,\n",
    "        replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=\"future\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
    "    # eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "    # eval_envs = HERGoalEnvWrapper(eval_envs)\n",
    "\n",
    "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
    "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
    "    # TrialEvalCallback signature:\n",
    "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(0) #replace with \"nan\" if error\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 07:50:58,837] A new study created in memory with name: no-name-a880c983-2a93-44ab-860e-9302f29d7ea9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9993421790412854, 'tau': 0.9771043197676195, 'batch_size': 103, 'buffer_size': 170800, 'learning_rate': 2.7575589792046808e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.compute_reward` for environment variables or `env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "[I 2024-06-20 07:55:31,310] Trial 0 finished with value: -50.0 and parameters: {'gamma': 0.0006578209587146004, 'tau': 0.9771043197676195, 'batch_size': 103, 'buffer_size': 170800, 'lr': 2.7575589792046808e-05, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 0 with value: -50.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9981636207570014, 'tau': 0.8736350672786739, 'batch_size': 65, 'buffer_size': 159676, 'learning_rate': 0.0037163038232821874, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:01:10,605] Trial 1 finished with value: -49.9 and parameters: {'gamma': 0.0018363792429986101, 'tau': 0.8736350672786739, 'batch_size': 65, 'buffer_size': 159676, 'lr': 0.0037163038232821874, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 1 with value: -49.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9889489387083114, 'tau': 0.5787070426665084, 'batch_size': 126, 'buffer_size': 890228, 'learning_rate': 0.001299077057406192, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:06:21,360] Trial 2 finished with value: -50.0 and parameters: {'gamma': 0.011051061291688674, 'tau': 0.5787070426665084, 'batch_size': 126, 'buffer_size': 890228, 'lr': 0.001299077057406192, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 1 with value: -49.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9969901634402519, 'tau': 0.4723175464598196, 'batch_size': 167, 'buffer_size': 623006, 'learning_rate': 0.00023576016907114202, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:11:49,861] Trial 3 finished with value: -49.8 and parameters: {'gamma': 0.00300983655974806, 'tau': 0.4723175464598196, 'batch_size': 167, 'buffer_size': 623006, 'lr': 0.00023576016907114202, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 3 with value: -49.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9997854971316794, 'tau': 0.582408269207137, 'batch_size': 165, 'buffer_size': 59439, 'learning_rate': 0.0016022597348332671, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:18:19,884] Trial 4 finished with value: -49.7 and parameters: {'gamma': 0.0002145028683206174, 'tau': 0.582408269207137, 'batch_size': 165, 'buffer_size': 59439, 'lr': 0.0016022597348332671, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 4 with value: -49.7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9998667448390275, 'tau': 0.03988280159549007, 'batch_size': 248, 'buffer_size': 13898, 'learning_rate': 0.9082486246118957, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:26:09,714] Trial 5 finished with value: -49.9 and parameters: {'gamma': 0.00013325516097256092, 'tau': 0.03988280159549007, 'batch_size': 248, 'buffer_size': 13898, 'lr': 0.9082486246118957, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 4 with value: -49.7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9554270917671509, 'tau': 0.4470088843189317, 'batch_size': 192, 'buffer_size': 21921, 'learning_rate': 0.05931252691751859, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:29:23,897] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9998580512692412, 'tau': 0.18265605524053963, 'batch_size': 217, 'buffer_size': 45430, 'learning_rate': 0.0193551203736735, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:32:52,627] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9994867711908918, 'tau': 0.7184766298996259, 'batch_size': 146, 'buffer_size': 65314, 'learning_rate': 1.5101865299199116e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:35:35,008] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9893752247828784, 'tau': 0.2907422751171632, 'batch_size': 185, 'buffer_size': 252556, 'learning_rate': 0.0002724579501831605, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:42:08,088] Trial 9 finished with value: -47.5 and parameters: {'gamma': 0.010624775217121617, 'tau': 0.2907422751171632, 'batch_size': 185, 'buffer_size': 252556, 'lr': 0.0002724579501831605, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 9 with value: -47.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9170265387184782, 'tau': 0.2956966805061621, 'batch_size': 253, 'buffer_size': 383959, 'learning_rate': 0.00016161950011335497, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:45:20,870] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9868794811173481, 'tau': 0.29341088085012457, 'batch_size': 186, 'buffer_size': 234449, 'learning_rate': 0.0006312381463305594, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:48:36,800] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9877646246358817, 'tau': 0.670309870955291, 'batch_size': 210, 'buffer_size': 77994, 'learning_rate': 0.006399828578318492, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:55:40,872] Trial 12 finished with value: -49.9 and parameters: {'gamma': 0.01223537536411835, 'tau': 0.670309870955291, 'batch_size': 210, 'buffer_size': 77994, 'lr': 0.006399828578318492, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 9 with value: -47.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9990944684881502, 'tau': 0.3491613774524635, 'batch_size': 150, 'buffer_size': 32384, 'learning_rate': 0.00010067429457294788, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 08:58:35,434] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9926782028606567, 'tau': 0.11434436335154974, 'batch_size': 118, 'buffer_size': 117062, 'learning_rate': 0.0008687854268982761, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:04:31,528] Trial 14 finished with value: -6.3 and parameters: {'gamma': 0.007321797139343367, 'tau': 0.11434436335154974, 'batch_size': 118, 'buffer_size': 117062, 'lr': 0.0008687854268982761, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 14 with value: -6.3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9729841735896851, 'tau': 0.018939803474223638, 'batch_size': 106, 'buffer_size': 329977, 'learning_rate': 4.4988103303741616e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:09:45,899] Trial 15 finished with value: -49.4 and parameters: {'gamma': 0.027015826410314837, 'tau': 0.018939803474223638, 'batch_size': 106, 'buffer_size': 329977, 'lr': 4.4988103303741616e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 14 with value: -6.3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.994162681548723, 'tau': 0.13783773958967338, 'batch_size': 66, 'buffer_size': 121834, 'learning_rate': 0.0006721712828767383, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:12:28,718] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9950724713552999, 'tau': 0.1778128550388754, 'batch_size': 124, 'buffer_size': 471217, 'learning_rate': 0.09050895784707208, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:15:30,773] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9713044031236061, 'tau': 0.363286658185408, 'batch_size': 92, 'buffer_size': 261995, 'learning_rate': 0.009631145165340266, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:17:55,567] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9984208783675806, 'tau': 0.1189573920401123, 'batch_size': 225, 'buffer_size': 106601, 'learning_rate': 0.0005184723309118714, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:24:56,962] Trial 19 finished with value: -50.0 and parameters: {'gamma': 0.0015791216324193974, 'tau': 0.1189573920401123, 'batch_size': 225, 'buffer_size': 106601, 'lr': 0.0005184723309118714, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 14 with value: -6.3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9939289799155016, 'tau': 0.24737205536138462, 'batch_size': 135, 'buffer_size': 197459, 'learning_rate': 9.72142967377655e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:30:45,610] Trial 20 finished with value: -49.8 and parameters: {'gamma': 0.006071020084498417, 'tau': 0.24737205536138462, 'batch_size': 135, 'buffer_size': 197459, 'lr': 9.72142967377655e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 14 with value: -6.3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9795417751356246, 'tau': 0.02224675072008557, 'batch_size': 102, 'buffer_size': 347882, 'learning_rate': 5.092629894170653e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:33:23,298] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9568820882148866, 'tau': 0.02059941354020507, 'batch_size': 83, 'buffer_size': 633890, 'learning_rate': 1.1155598720031893e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:38:24,739] Trial 22 finished with value: -49.5 and parameters: {'gamma': 0.043117911785113344, 'tau': 0.02059941354020507, 'batch_size': 83, 'buffer_size': 633890, 'lr': 1.1155598720031893e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 14 with value: -6.3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9086624838023981, 'tau': 0.10317294458722988, 'batch_size': 118, 'buffer_size': 281886, 'learning_rate': 0.00029407631293953474, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:44:09,982] Trial 23 finished with value: -1.8 and parameters: {'gamma': 0.09133751619760186, 'tau': 0.10317294458722988, 'batch_size': 118, 'buffer_size': 281886, 'lr': 0.00029407631293953474, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9043338663277906, 'tau': 0.21450905272680937, 'batch_size': 183, 'buffer_size': 134162, 'learning_rate': 0.0003606548274232816, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:47:23,171] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9969265644411075, 'tau': 0.3584138828603502, 'batch_size': 119, 'buffer_size': 249597, 'learning_rate': 0.0021004846531533306, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:50:28,743] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9914799342876678, 'tau': 0.1189224117261572, 'batch_size': 145, 'buffer_size': 97812, 'learning_rate': 0.0010477688122085696, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:53:33,711] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9803153188745756, 'tau': 0.09858786351970528, 'batch_size': 168, 'buffer_size': 925776, 'learning_rate': 0.00018129738108600772, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:56:24,516] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9475324687633745, 'tau': 0.26253988662831107, 'batch_size': 197, 'buffer_size': 442506, 'learning_rate': 0.003954777869589844, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 09:59:58,316] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9986155760664499, 'tau': 0.3971894722920959, 'batch_size': 111, 'buffer_size': 181254, 'learning_rate': 3.559700509958445e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:02:16,211] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9954828913243906, 'tau': 0.21570997154726618, 'batch_size': 79, 'buffer_size': 153852, 'learning_rate': 9.494414821365702e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:07:18,441] Trial 30 finished with value: -50.0 and parameters: {'gamma': 0.004517108675609418, 'tau': 0.21570997154726618, 'batch_size': 79, 'buffer_size': 153852, 'lr': 9.494414821365702e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9727331530365957, 'tau': 0.04421319058345419, 'batch_size': 98, 'buffer_size': 278935, 'learning_rate': 4.436153892998109e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:12:31,068] Trial 31 finished with value: -49.5 and parameters: {'gamma': 0.02726684696340431, 'tau': 0.04421319058345419, 'batch_size': 98, 'buffer_size': 278935, 'lr': 4.436153892998109e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9817710777301679, 'tau': 0.9946096052047901, 'batch_size': 111, 'buffer_size': 348936, 'learning_rate': 2.46873554405666e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:17:54,353] Trial 32 finished with value: -49.8 and parameters: {'gamma': 0.018228922269832044, 'tau': 0.9946096052047901, 'batch_size': 111, 'buffer_size': 348936, 'lr': 2.46873554405666e-05, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9913260721958915, 'tau': 0.0851842606515868, 'batch_size': 128, 'buffer_size': 567102, 'learning_rate': 0.0003246975517869488, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:20:52,523] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9373678844916954, 'tau': 0.006350720026833401, 'batch_size': 137, 'buffer_size': 187010, 'learning_rate': 9.106119421157285e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:23:36,319] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9681786711386203, 'tau': 0.8739669319616764, 'batch_size': 175, 'buffer_size': 307977, 'learning_rate': 0.0011940372760416492, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:26:20,246] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9976615441612903, 'tau': 0.1633129234186459, 'batch_size': 112, 'buffer_size': 82186, 'learning_rate': 0.00021709100395296224, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:29:04,938] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9821587616082714, 'tau': 0.07288971758997415, 'batch_size': 160, 'buffer_size': 620987, 'learning_rate': 0.003033735835978285, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:35:56,261] Trial 37 finished with value: -2.1 and parameters: {'gamma': 0.017841238391728593, 'tau': 0.07288971758997415, 'batch_size': 160, 'buffer_size': 620987, 'lr': 0.003033735835978285, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99084527663191, 'tau': 0.5548731003620411, 'batch_size': 155, 'buffer_size': 619258, 'learning_rate': 0.0026575713360567385, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:39:19,154] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9858353467563102, 'tau': 0.07169222919895943, 'batch_size': 203, 'buffer_size': 731729, 'learning_rate': 0.02309595688119062, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:42:52,349] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9963255823902618, 'tau': 0.30609642647519786, 'batch_size': 233, 'buffer_size': 45908, 'learning_rate': 0.5672197041270528, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:46:44,388] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9632659135679091, 'tau': 0.08455719058667434, 'batch_size': 162, 'buffer_size': 471301, 'learning_rate': 0.0009076956656881169, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:53:21,235] Trial 41 finished with value: -2.0 and parameters: {'gamma': 0.036734086432090876, 'tau': 0.08455719058667434, 'batch_size': 162, 'buffer_size': 471301, 'lr': 0.0009076956656881169, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 23 with value: -1.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9171504995346392, 'tau': 0.16834360944905058, 'batch_size': 165, 'buffer_size': 482032, 'learning_rate': 0.0009883701661572695, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 10:59:59,853] Trial 42 finished with value: -1.4 and parameters: {'gamma': 0.08284950046536081, 'tau': 0.16834360944905058, 'batch_size': 165, 'buffer_size': 482032, 'lr': 0.0009883701661572695, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9323686549961232, 'tau': 0.07054788300256916, 'batch_size': 162, 'buffer_size': 793231, 'learning_rate': 0.004134885350165996, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:03:31,263] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9625755435757257, 'tau': 0.19058418753405812, 'batch_size': 176, 'buffer_size': 489426, 'learning_rate': 0.0010199886568900357, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:10:20,931] Trial 44 finished with value: -7.5 and parameters: {'gamma': 0.0374244564242743, 'tau': 0.19058418753405812, 'batch_size': 176, 'buffer_size': 489426, 'lr': 0.0010199886568900357, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9106404175776678, 'tau': 0.1340060052712116, 'batch_size': 137, 'buffer_size': 444839, 'learning_rate': 0.008946380644719379, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:13:33,196] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9408283490939063, 'tau': 0.0661313982260346, 'batch_size': 155, 'buffer_size': 932958, 'learning_rate': 0.002184517224861537, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:16:57,670] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9631147163783274, 'tau': 0.24050299345014084, 'batch_size': 145, 'buffer_size': 725414, 'learning_rate': 0.0012560136119049439, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:20:14,340] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9257374377779553, 'tau': 0.1556603574929456, 'batch_size': 172, 'buffer_size': 14051, 'learning_rate': 0.0005150459476045427, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:26:29,164] Trial 48 finished with value: -49.8 and parameters: {'gamma': 0.0742625622220447, 'tau': 0.1556603574929456, 'batch_size': 172, 'buffer_size': 14051, 'lr': 0.0005150459476045427, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9526257569539471, 'tau': 0.44690645646371063, 'batch_size': 162, 'buffer_size': 542329, 'learning_rate': 0.019729898306758937, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:29:44,292] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9995783790779762, 'tau': 0.11288854624591244, 'batch_size': 129, 'buffer_size': 210105, 'learning_rate': 0.00568575940448975, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:32:31,355] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9840317815563451, 'tau': 0.19048410340809357, 'batch_size': 177, 'buffer_size': 471431, 'learning_rate': 0.0008705078838062499, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:39:21,589] Trial 51 finished with value: -49.8 and parameters: {'gamma': 0.01596821844365488, 'tau': 0.19048410340809357, 'batch_size': 177, 'buffer_size': 471431, 'lr': 0.0008705078838062499, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9624084831486707, 'tau': 0.18838813124611858, 'batch_size': 187, 'buffer_size': 371838, 'learning_rate': 0.0016847953448224447, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:42:48,310] Trial 52 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9737890797116572, 'tau': 0.05240903591726909, 'batch_size': 195, 'buffer_size': 417153, 'learning_rate': 0.00043202741555520275, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:46:12,994] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9768557561478056, 'tau': 0.7078058196722774, 'batch_size': 153, 'buffer_size': 592202, 'learning_rate': 0.000831112045544254, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:49:32,008] Trial 54 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9031476611846615, 'tau': 0.15010194275934619, 'batch_size': 180, 'buffer_size': 519153, 'learning_rate': 0.003345204251814696, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 11:56:39,695] Trial 55 finished with value: -49.9 and parameters: {'gamma': 0.0968523388153385, 'tau': 0.15010194275934619, 'batch_size': 180, 'buffer_size': 519153, 'lr': 0.003345204251814696, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9497452155038956, 'tau': 0.2159237206805858, 'batch_size': 165, 'buffer_size': 753358, 'learning_rate': 0.0006930105866234027, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:03:28,817] Trial 56 finished with value: -24.8 and parameters: {'gamma': 0.05025478449610444, 'tau': 0.2159237206805858, 'batch_size': 165, 'buffer_size': 753358, 'lr': 0.0006930105866234027, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9644752118280606, 'tau': 0.106665756247004, 'batch_size': 118, 'buffer_size': 300077, 'learning_rate': 0.0016510348778117207, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:06:29,341] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9367698223761131, 'tau': 0.2833438722999369, 'batch_size': 209, 'buffer_size': 388492, 'learning_rate': 0.0001454113477043644, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:13:06,014] Trial 58 finished with value: -49.8 and parameters: {'gamma': 0.06323017762388697, 'tau': 0.2833438722999369, 'batch_size': 209, 'buffer_size': 388492, 'lr': 0.0001454113477043644, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9564951458052655, 'tau': 0.039554133774145406, 'batch_size': 142, 'buffer_size': 26327, 'learning_rate': 0.00028658236128472297, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:18:55,989] Trial 59 finished with value: -1.5 and parameters: {'gamma': 0.04350485419473449, 'tau': 0.039554133774145406, 'batch_size': 142, 'buffer_size': 26327, 'lr': 0.00028658236128472297, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9219133607359298, 'tau': 0.016694567057404824, 'batch_size': 143, 'buffer_size': 22968, 'learning_rate': 0.00028925177060813126, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:24:44,024] Trial 60 finished with value: -2.0 and parameters: {'gamma': 0.07808663926407025, 'tau': 0.016694567057404824, 'batch_size': 143, 'buffer_size': 22968, 'lr': 0.00028925177060813126, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9181191052651965, 'tau': 0.019639814067384034, 'batch_size': 141, 'buffer_size': 24015, 'learning_rate': 0.0002633501069722777, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:30:29,502] Trial 61 finished with value: -13.2 and parameters: {'gamma': 0.08188089473480359, 'tau': 0.019639814067384034, 'batch_size': 141, 'buffer_size': 24015, 'lr': 0.0002633501069722777, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9517321836722811, 'tau': 0.04678799024781944, 'batch_size': 132, 'buffer_size': 11322, 'learning_rate': 0.00038394185736277594, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:36:16,811] Trial 62 finished with value: -1.7 and parameters: {'gamma': 0.04826781632771892, 'tau': 0.04678799024781944, 'batch_size': 132, 'buffer_size': 11322, 'lr': 0.00038394185736277594, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9512779795189696, 'tau': 0.004655444333295497, 'batch_size': 150, 'buffer_size': 10532, 'learning_rate': 0.00017747926802085998, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:42:00,379] Trial 63 finished with value: -37.6 and parameters: {'gamma': 0.048722020481030366, 'tau': 0.004655444333295497, 'batch_size': 150, 'buffer_size': 10532, 'lr': 0.00017747926802085998, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9195667772227979, 'tau': 0.05454044418982642, 'batch_size': 132, 'buffer_size': 18076, 'learning_rate': 0.00038990661378633695, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:47:45,820] Trial 64 finished with value: -1.7 and parameters: {'gamma': 0.08043322277720208, 'tau': 0.05454044418982642, 'batch_size': 132, 'buffer_size': 18076, 'lr': 0.00038990661378633695, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.925826747850386, 'tau': 0.03130795457758159, 'batch_size': 131, 'buffer_size': 18722, 'learning_rate': 0.0004000543417563579, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:53:36,552] Trial 65 finished with value: -2.0 and parameters: {'gamma': 0.07417325214961405, 'tau': 0.03130795457758159, 'batch_size': 131, 'buffer_size': 18722, 'lr': 0.0004000543417563579, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9998968674150953, 'tau': 0.044350976276631544, 'batch_size': 124, 'buffer_size': 28875, 'learning_rate': 0.0003028026328828397, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 12:56:25,324] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9000080733269553, 'tau': 0.10425979488868556, 'batch_size': 144, 'buffer_size': 17200, 'learning_rate': 0.0001251164351248088, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:02:02,933] Trial 67 finished with value: -40.2 and parameters: {'gamma': 0.0999919266730447, 'tau': 0.10425979488868556, 'batch_size': 144, 'buffer_size': 17200, 'lr': 0.0001251164351248088, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9423260281053331, 'tau': 0.14077757918975078, 'batch_size': 120, 'buffer_size': 11006, 'learning_rate': 0.00046751414768094184, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:04:23,401] Trial 68 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9562420075750696, 'tau': 0.09216938843198405, 'batch_size': 138, 'buffer_size': 13435, 'learning_rate': 7.019150314602951e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:07:06,719] Trial 69 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9270956708770977, 'tau': 0.0035808924123816657, 'batch_size': 133, 'buffer_size': 33741, 'learning_rate': 0.00022363159687013254, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:12:46,407] Trial 70 finished with value: -31.8 and parameters: {'gamma': 0.07290432912290233, 'tau': 0.0035808924123816657, 'batch_size': 133, 'buffer_size': 33741, 'lr': 0.00022363159687013254, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9212471986654672, 'tau': 0.04051128326519404, 'batch_size': 131, 'buffer_size': 18391, 'learning_rate': 0.00035924994843767, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:18:34,409] Trial 71 finished with value: -1.6 and parameters: {'gamma': 0.07875280133453279, 'tau': 0.04051128326519404, 'batch_size': 131, 'buffer_size': 18391, 'lr': 0.00035924994843767, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.918133272018056, 'tau': 0.046501958600875505, 'batch_size': 149, 'buffer_size': 21144, 'learning_rate': 0.000639984077110016, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:24:38,618] Trial 72 finished with value: -1.4 and parameters: {'gamma': 0.08186672798194403, 'tau': 0.046501958600875505, 'batch_size': 149, 'buffer_size': 21144, 'lr': 0.000639984077110016, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9431313998214501, 'tau': 0.06056482455120696, 'batch_size': 123, 'buffer_size': 17568, 'learning_rate': 0.0005641493122587994, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:30:28,558] Trial 73 finished with value: -1.8 and parameters: {'gamma': 0.05686860017854987, 'tau': 0.06056482455120696, 'batch_size': 123, 'buffer_size': 17568, 'lr': 0.0005641493122587994, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9476458129677343, 'tau': 0.046943603502593294, 'batch_size': 124, 'buffer_size': 17391, 'learning_rate': 0.0001339055395680442, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:36:01,437] Trial 74 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9370634150574161, 'tau': 0.04379677144849251, 'batch_size': 102, 'buffer_size': 12906, 'learning_rate': 0.0005966136153529288, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:41:41,356] Trial 75 finished with value: -1.6 and parameters: {'gamma': 0.06293658494258388, 'tau': 0.04379677144849251, 'batch_size': 102, 'buffer_size': 12906, 'lr': 0.0005966136153529288, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9154500734505773, 'tau': 0.7996545202271448, 'batch_size': 94, 'buffer_size': 12681, 'learning_rate': 0.000672849860814791, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:44:33,027] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.932675009407072, 'tau': 0.039211594049029526, 'batch_size': 103, 'buffer_size': 19939, 'learning_rate': 0.0003783354350118757, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:50:04,624] Trial 77 finished with value: -1.9 and parameters: {'gamma': 0.06732499059292794, 'tau': 0.039211594049029526, 'batch_size': 103, 'buffer_size': 19939, 'lr': 0.0003783354350118757, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9698704294821988, 'tau': 0.1283378397267332, 'batch_size': 106, 'buffer_size': 15190, 'learning_rate': 6.112201791752937e-05, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:52:39,185] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9575316017334744, 'tau': 0.16776812381384063, 'batch_size': 78, 'buffer_size': 11755, 'learning_rate': 0.00017421769229896151, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 13:54:47,198] Trial 79 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.90033053585047, 'tau': 0.09459144887728466, 'batch_size': 116, 'buffer_size': 15436, 'learning_rate': 0.001392732362807742, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:00:48,939] Trial 80 finished with value: -50.0 and parameters: {'gamma': 0.09966946414953004, 'tau': 0.09459144887728466, 'batch_size': 116, 'buffer_size': 15436, 'lr': 0.001392732362807742, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9418008832938216, 'tau': 0.06798424570738215, 'batch_size': 113, 'buffer_size': 29329, 'learning_rate': 0.0006530764378027246, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:06:33,379] Trial 81 finished with value: -1.6 and parameters: {'gamma': 0.05819911670617842, 'tau': 0.06798424570738215, 'batch_size': 113, 'buffer_size': 29329, 'lr': 0.0006530764378027246, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9340262771851151, 'tau': 0.0770714310436106, 'batch_size': 115, 'buffer_size': 27437, 'learning_rate': 0.0006022887427362706, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:12:16,613] Trial 82 finished with value: -1.6 and parameters: {'gamma': 0.06597372281488491, 'tau': 0.0770714310436106, 'batch_size': 115, 'buffer_size': 27437, 'lr': 0.0006022887427362706, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9364290834718976, 'tau': 0.06764547780677545, 'batch_size': 87, 'buffer_size': 28088, 'learning_rate': 0.0006451169137760629, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:15:03,210] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9212759808411716, 'tau': 0.0020588909645920667, 'batch_size': 110, 'buffer_size': 37472, 'learning_rate': 0.00020972864818053324, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:20:27,605] Trial 84 finished with value: -3.5 and parameters: {'gamma': 0.0787240191588284, 'tau': 0.0020588909645920667, 'batch_size': 110, 'buffer_size': 37472, 'lr': 0.00020972864818053324, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9563588271267642, 'tau': 0.038889836197462085, 'batch_size': 98, 'buffer_size': 25224, 'learning_rate': 0.000522191442323342, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:26:04,287] Trial 85 finished with value: -1.6 and parameters: {'gamma': 0.04364117287323576, 'tau': 0.038889836197462085, 'batch_size': 98, 'buffer_size': 25224, 'lr': 0.000522191442323342, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9782812194221814, 'tau': 0.03613459730674251, 'batch_size': 97, 'buffer_size': 47108, 'learning_rate': 0.0018693578776565007, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:32:00,402] Trial 86 finished with value: -2.0 and parameters: {'gamma': 0.021718780577818626, 'tau': 0.03613459730674251, 'batch_size': 97, 'buffer_size': 47108, 'lr': 0.0018693578776565007, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 42 with value: -1.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9565727012599055, 'tau': 0.08975927670697581, 'batch_size': 149, 'buffer_size': 56495, 'learning_rate': 0.001112571800254993, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:38:20,322] Trial 87 finished with value: -1.2 and parameters: {'gamma': 0.04342729874009449, 'tau': 0.08975927670697581, 'batch_size': 149, 'buffer_size': 56495, 'lr': 0.001112571800254993, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 87 with value: -1.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9689348626439084, 'tau': 0.11924806800834153, 'batch_size': 150, 'buffer_size': 39066, 'learning_rate': 0.0011144596207549467, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:44:35,501] Trial 88 finished with value: -2.1 and parameters: {'gamma': 0.031065137356091663, 'tau': 0.11924806800834153, 'batch_size': 150, 'buffer_size': 39066, 'lr': 0.0011144596207549467, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 87 with value: -1.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9573253335483596, 'tau': 0.08494234264837025, 'batch_size': 170, 'buffer_size': 55074, 'learning_rate': 0.0007544071494407633, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:50:56,545] Trial 89 finished with value: -1.5 and parameters: {'gamma': 0.04267466645164039, 'tau': 0.08494234264837025, 'batch_size': 170, 'buffer_size': 55074, 'lr': 0.0007544071494407633, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 87 with value: -1.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9352352182957356, 'tau': 0.08165502676699067, 'batch_size': 157, 'buffer_size': 20855, 'learning_rate': 0.0024609305127437663, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 14:57:20,144] Trial 90 finished with value: -1.6 and parameters: {'gamma': 0.06476478170426439, 'tau': 0.08165502676699067, 'batch_size': 157, 'buffer_size': 20855, 'lr': 0.0024609305127437663, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 87 with value: -1.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9587996739978939, 'tau': 0.13898619916693722, 'batch_size': 100, 'buffer_size': 54746, 'learning_rate': 0.0008130885677121134, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:00:07,580] Trial 91 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9459971184617512, 'tau': 0.08568714420335825, 'batch_size': 108, 'buffer_size': 25034, 'learning_rate': 0.0005475291257802596, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:02:55,703] Trial 92 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9997650138567994, 'tau': 0.6194685296369983, 'batch_size': 172, 'buffer_size': 27987, 'learning_rate': 0.001382306851812352, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:06:10,117] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9663231953177872, 'tau': 0.16129189553056603, 'batch_size': 90, 'buffer_size': 59449, 'learning_rate': 0.0007679348979490404, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:08:56,254] Trial 94 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9741273742139606, 'tau': 0.022108947086237923, 'batch_size': 166, 'buffer_size': 85344, 'learning_rate': 0.0009678496054008291, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:15:27,842] Trial 95 finished with value: -1.1 and parameters: {'gamma': 0.025872625786039386, 'tau': 0.022108947086237923, 'batch_size': 166, 'buffer_size': 85344, 'lr': 0.0009678496054008291, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9747409676944536, 'tau': 0.0705627188640863, 'batch_size': 168, 'buffer_size': 94106, 'learning_rate': 0.0009489806993665459, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:21:49,287] Trial 96 finished with value: -10.0 and parameters: {'gamma': 0.025259032305546342, 'tau': 0.0705627188640863, 'batch_size': 168, 'buffer_size': 94106, 'lr': 0.0009489806993665459, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9438972629291359, 'tau': 0.02599133450954895, 'batch_size': 148, 'buffer_size': 72417, 'learning_rate': 0.001099339804985099, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:24:19,372] Trial 97 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9309264366581584, 'tau': 0.12063358039034416, 'batch_size': 114, 'buffer_size': 52745, 'learning_rate': 0.0015639911891133622, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:30:10,606] Trial 98 finished with value: -1.9 and parameters: {'gamma': 0.06907356334184153, 'tau': 0.12063358039034416, 'batch_size': 114, 'buffer_size': 52745, 'lr': 0.0015639911891133622, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9582014123150074, 'tau': 0.0006003083568414114, 'batch_size': 158, 'buffer_size': 40574, 'learning_rate': 0.0002489408713045189, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:33:08,604] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9697095457202344, 'tau': 0.10008580067408342, 'batch_size': 139, 'buffer_size': 85434, 'learning_rate': 0.0006345525717048618, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:36:09,541] Trial 100 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9483351897750566, 'tau': 0.03023784175028729, 'batch_size': 163, 'buffer_size': 68319, 'learning_rate': 0.0005340586259983456, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:39:18,378] Trial 101 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9598805413779483, 'tau': 0.06041932433396108, 'batch_size': 167, 'buffer_size': 31864, 'learning_rate': 0.00047782643027392276, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:42:24,060] Trial 102 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9990389474600262, 'tau': 0.0856699989266727, 'batch_size': 171, 'buffer_size': 25776, 'learning_rate': 0.0019492814986765213, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:45:34,083] Trial 103 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9372409758104752, 'tau': 0.023004282594819105, 'batch_size': 188, 'buffer_size': 139808, 'learning_rate': 0.0003134161695218845, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:51:57,125] Trial 104 finished with value: -1.4 and parameters: {'gamma': 0.06275902418952481, 'tau': 0.023004282594819105, 'batch_size': 188, 'buffer_size': 139808, 'lr': 0.0003134161695218845, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9377272238359134, 'tau': 0.01660257000383089, 'batch_size': 192, 'buffer_size': 111261, 'learning_rate': 0.0003240685252919863, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 15:58:26,113] Trial 105 finished with value: -1.9 and parameters: {'gamma': 0.06227277616408655, 'tau': 0.01660257000383089, 'batch_size': 192, 'buffer_size': 111261, 'lr': 0.0003240685252919863, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9077620261550544, 'tau': 0.06842888110986958, 'batch_size': 183, 'buffer_size': 33033, 'learning_rate': 0.15329271230402572, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:01:59,488] Trial 106 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9270249708886451, 'tau': 0.5371385530259811, 'batch_size': 151, 'buffer_size': 129066, 'learning_rate': 0.001270903705470964, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:05:11,026] Trial 107 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.914848917047551, 'tau': 0.10806184840773272, 'batch_size': 188, 'buffer_size': 150139, 'learning_rate': 0.0008777166801538266, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:11:49,404] Trial 108 finished with value: -2.0 and parameters: {'gamma': 0.08515108295244896, 'tau': 0.10806184840773272, 'batch_size': 188, 'buffer_size': 150139, 'lr': 0.0008777166801538266, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9513724651286738, 'tau': 0.1361905998755782, 'batch_size': 178, 'buffer_size': 61448, 'learning_rate': 0.004552619895086027, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:15:11,837] Trial 109 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9665940652584248, 'tau': 0.21268610674286573, 'batch_size': 127, 'buffer_size': 43883, 'learning_rate': 0.00011139201610971199, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:17:51,134] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9524779539285079, 'tau': 0.036264880825164436, 'batch_size': 154, 'buffer_size': 26390, 'learning_rate': 0.000328660599514793, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:20:45,055] Trial 111 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9420006870346187, 'tau': 0.0199198104251322, 'batch_size': 147, 'buffer_size': 20905, 'learning_rate': 0.0006871428423371676, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:23:43,565] Trial 112 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.932412528597207, 'tau': 0.057059240471336724, 'batch_size': 141, 'buffer_size': 22370, 'learning_rate': 0.00043474404235700927, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:26:37,642] Trial 113 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9569523304294899, 'tau': 0.04258967446263765, 'batch_size': 105, 'buffer_size': 31146, 'learning_rate': 0.0005191693498553302, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:32:10,505] Trial 114 finished with value: -1.5 and parameters: {'gamma': 0.043047669570510136, 'tau': 0.04258967446263765, 'batch_size': 105, 'buffer_size': 31146, 'lr': 0.0005191693498553302, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9134112102551499, 'tau': 0.08627950812702342, 'batch_size': 106, 'buffer_size': 36198, 'learning_rate': 0.00022050705350569837, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:34:49,242] Trial 115 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9756763856957964, 'tau': 0.05510824486199492, 'batch_size': 158, 'buffer_size': 30469, 'learning_rate': 0.0002635608808880127, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:37:17,184] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9630805847613292, 'tau': 0.40220227724958896, 'batch_size': 201, 'buffer_size': 15538, 'learning_rate': 0.0010349377422742697, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:43:58,081] Trial 117 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9427276070481864, 'tau': 0.023458774049913922, 'batch_size': 114, 'buffer_size': 50096, 'learning_rate': 0.002646356259094869, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:50:03,245] Trial 118 finished with value: -1.7 and parameters: {'gamma': 0.05727239295181359, 'tau': 0.023458774049913922, 'batch_size': 114, 'buffer_size': 50096, 'lr': 0.002646356259094869, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9725553330812367, 'tau': 0.11350846402598769, 'batch_size': 174, 'buffer_size': 42528, 'learning_rate': 0.0004410199798682519, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:53:12,936] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9242267265543661, 'tau': 0.9383353254273502, 'batch_size': 105, 'buffer_size': 77837, 'learning_rate': 0.0006988886815224243, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:56:09,248] Trial 120 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9546296379047019, 'tau': 1.9563733006822126e-05, 'batch_size': 99, 'buffer_size': 23401, 'learning_rate': 0.0004752496094606763, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:58:56,540] Trial 121 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9554150843054694, 'tau': 0.037429311249442, 'batch_size': 94, 'buffer_size': 19496, 'learning_rate': 0.000340752695536061, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:01:39,277] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9623160890827773, 'tau': 0.08089090882361019, 'batch_size': 82, 'buffer_size': 35001, 'learning_rate': 0.000884861439546227, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:04:26,969] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.936530709864527, 'tau': 0.053727983921251214, 'batch_size': 120, 'buffer_size': 30294, 'learning_rate': 0.000522324718616476, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:10:12,779] Trial 124 finished with value: -1.7 and parameters: {'gamma': 0.06346929013547295, 'tau': 0.053727983921251214, 'batch_size': 120, 'buffer_size': 30294, 'lr': 0.000522324718616476, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9439688582834641, 'tau': 0.041443667686110866, 'batch_size': 102, 'buffer_size': 26198, 'learning_rate': 0.0006336352382864366, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:15:52,050] Trial 125 finished with value: -1.8 and parameters: {'gamma': 0.056031141716535914, 'tau': 0.041443667686110866, 'batch_size': 102, 'buffer_size': 26198, 'lr': 0.0006336352382864366, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9152305258776386, 'tau': 0.1003433130971848, 'batch_size': 109, 'buffer_size': 95127, 'learning_rate': 0.00016491690097796837, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:18:34,380] Trial 126 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9491856753000525, 'tau': 0.0772049012316129, 'batch_size': 90, 'buffer_size': 15951, 'learning_rate': 0.0013050377371237316, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:24:21,036] Trial 127 finished with value: -49.9 and parameters: {'gamma': 0.050814324699947525, 'tau': 0.0772049012316129, 'batch_size': 90, 'buffer_size': 15951, 'lr': 0.0013050377371237316, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9805863043610494, 'tau': 0.15331714653657935, 'batch_size': 161, 'buffer_size': 220347, 'learning_rate': 0.0002753449392596498, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:27:26,471] Trial 128 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9978208265174625, 'tau': 0.12540336364836197, 'batch_size': 168, 'buffer_size': 21880, 'learning_rate': 0.0007518957373464678, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:30:35,776] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9309979220783098, 'tau': 0.01636650516881294, 'batch_size': 135, 'buffer_size': 12425, 'learning_rate': 0.0003885903410200412, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 17:36:31,964] Trial 130 finished with value: -2.2 and parameters: {'gamma': 0.06900207792169018, 'tau': 0.01636650516881294, 'batch_size': 135, 'buffer_size': 12425, 'lr': 0.0003885903410200412, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 95 with value: -1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9343247725060793, 'tau': 0.07925201707711486, 'batch_size': 155, 'buffer_size': 20951, 'learning_rate': 0.0015703877034023623, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>}}\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "th.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS) # timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_ddpg_fetchreach.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DDPG_HER Implementation from scratch using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gym.wrappers import FilterObservation, FlattenObservation, TransformObservation\n",
    "from gym import spaces\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v2')\n",
    "env.reset()\n",
    "\n",
    "# run the agent for 200 iterations in the environment\n",
    "for i in range(200):\n",
    "    # do not use env.render() with jupyter notebook\n",
    "    # env.render() \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, x, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpackObs(obs):\n",
    "    return  obs[0]['achieved_goal'], \\\n",
    "            obs[0]['desired_goal'],\\\n",
    "            np.concatenate((obs[0]['observation'], \\\n",
    "            obs[0]['desired_goal'])), \\\n",
    "            np.concatenate((obs[0]['observation'], \\\n",
    "            obs[0]['achieved_goal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpackObs_new(obs):\n",
    "    return  obs['achieved_goal'], \\\n",
    "            obs['desired_goal'],\\\n",
    "            np.concatenate((obs['observation'], \\\n",
    "            obs['desired_goal'])), \\\n",
    "            np.concatenate((obs['observation'], \\\n",
    "            obs['achieved_goal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpackObs(env.reset())\n",
    "assert(len(unpackObs(env.reset())) == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpackObs(env.reset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1,1), np.array(d).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\n",
    "\tdef __init__(self, action_size, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "\t\tself.action_size = action_size\n",
    "\t\tself.mu = mu\n",
    "\t\tself.theta = theta\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.X = np.ones(self.action_size) * self.mu\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.X = np.ones(self.action_size) * self.mu\n",
    "\n",
    "\tdef sample(self):\n",
    "\t\tdx = self.theta * (self.mu - self.X)\n",
    "\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "\t\tself.X = self.X + dx\n",
    "\t\tself.X = [abs(x) for x in self.X]\n",
    "\t\treturn self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = OrnsteinUhlenbeckActionNoise(4).sample()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat Actor NN\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size, action_max):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.input_layer = nn.LayerNorm(state_size)\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_1n = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_layer_2n = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "#         nn.init.uniform_(self.dense_layer_1.weight)\n",
    "#         nn.init.uniform_(self.dense_layer_2.weight)\n",
    "        nn.init.uniform_(self.output.weight,-0.003,0.003)\n",
    "        \n",
    "        self.action_max = action_max\n",
    "     \n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = torch.clamp(x,-1.0,1.0)\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(self.dropout1(self.dense_layer_1n(self.dense_layer_1(x))))\n",
    "        x = F.relu(self.dropout2(self.dense_layer_2n(self.dense_layer_2(x))))\n",
    "        return torch.tanh(self.output(x)) * self.action_max\n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.input_layer = nn.LayerNorm(state_size+action_size)\n",
    "        self.dense_layer_1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "        self.dense_layer_1n = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_layer_2n = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dense_layer_3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_layer_3n = nn.LayerNorm(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "#         nn.init.uniform_(self.dense_layer_1.weight)\n",
    "#         nn.init.uniform_(self.dense_layer_2.weight)\n",
    "        nn.init.uniform_(self.output.weight,-0.003,0.003)\n",
    "        \n",
    "\n",
    "    def forward(self, x, a):\n",
    "#         x = torch.clamp(x,-1.0,1.0)\n",
    "        x = self.input_layer(torch.cat((x,a),dim=1))\n",
    "        x = F.relu(self.dropout1(self.dense_layer_1n(self.dense_layer_1(x))))\n",
    "        x = F.relu(self.dropout2(self.dense_layer_2n(self.dense_layer_2(x))))\n",
    "        x = F.relu(self.dropout3(self.dense_layer_3n(self.dense_layer_3(x))))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    def __init__(self, state_size, action_size, hidden_size, actor_lr, critic_lr, discount,\n",
    "                 min_action, max_action, exploration_noise):\n",
    "        self.action_size = action_size\n",
    "        self.actor = ActorNet(state_size, action_size, hidden_size, max_action).to(device)\n",
    "        self.actor_target = ActorNet(state_size, action_size, hidden_size, max_action).to(device)\n",
    "        \n",
    "        self.critic = CriticNet(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic_target = CriticNet(state_size, action_size, hidden_size).to(device)\n",
    "        \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.discount = discount\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        self.exploration_noise = exploration_noise\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        #get action probs then randomly sample from the probabilities\n",
    "        with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(input_state)\n",
    "            #detach and turn to numpy to use with np.random.choice()\n",
    "            action = action.detach().cpu().numpy()\n",
    "            #in DDPG add noise for exploration\n",
    "            action = (action + np.random.normal(0., self.exploration_noise, \n",
    "                       size=self.action_size)).clip(self.min_action, self.max_action)   \n",
    "        return action\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        # sample a batch from the replay buffer\n",
    "        x0, x1, a, r, d = replay_buffer.sample(batch_size)\n",
    "        # turn batches into tensors and use GPU if available\n",
    "        state_batch = torch.FloatTensor(x0[:,3:]).to(device)\n",
    "        next_state_batch = torch.FloatTensor(x1[:,3:]).to(device)\n",
    "        action_batch = torch.FloatTensor(a).to(device)\n",
    "        reward_batch = torch.FloatTensor(r).to(device)\n",
    "        flipped_done_batch = torch.FloatTensor(d).to(device) #already flipped done when adding to replay buffer\n",
    "\n",
    "        # get target net target values\n",
    "        with torch.no_grad():\n",
    "            target_action = self.actor_target(next_state_batch).view(batch_size,-1)\n",
    "            target_v = reward_batch + flipped_done_batch*self.discount*self.critic_target(next_state_batch, \n",
    "                                                                            target_action).view(batch_size,-1) # this is the expected reward that we are modelling using markovian process(bellman's curve)\n",
    "        # get train net values for updating the critic network    \n",
    "            \n",
    "            critic_v = self.critic(state_batch, action_batch).view(batch_size,-1) # this is the reward that our critic(deep Q network) says will be received.\n",
    "        \n",
    "        # train critic\n",
    "        critic_loss = nn.MSELoss()(target_v, critic_v) # we take the mse between the actual value(target_v) and predicted value(critic_v)\n",
    "        critic_loss.requires_grad = True\n",
    "#         critic_loss = F.smooth_l1_loss(critic_v, target_v)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step() \n",
    "        \n",
    "        # train actor\n",
    "        train_action = self.actor(state_batch) #gets action to be taken for the respective state\n",
    "        # actor loss need to technically be -torch.mean(self.critic(state_batch,train_action)) but it seems to be working better with no - sign, not sure why\n",
    "        actor_loss = torch.mean(self.critic(state_batch,train_action)) # expectation value of the q values(rewards) calculated on the basis of the current state and the action to be taken on it. This action is calculated from the actor network.\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step() \n",
    "\n",
    "        torch.save(self.actor.state_dict(), '/teamspace/studios/this_studio/actor_model_GPU_1.pth')\n",
    "        torch.save(self.critic.state_dict(), '/teamspace/studios/this_studio/critic_model_GPU_1.pth')\n",
    "\n",
    "        \n",
    "        return actor_loss.detach().cpu().numpy(), critic_loss.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "    def update_target_network_soft(self, update_tau=0.001):\n",
    "        # soft target network update: update target networks with mixture of train and target\n",
    "        for target_var, var in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_var.data.copy_((1.-update_tau) * target_var.data + (update_tau) * var.data)\n",
    "            \n",
    "        for target_var, var in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_var.data.copy_((1.-update_tau) * target_var.data + (update_tau) * var.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v2')\n",
    "# env = TransformObservation(env, lambda obs: {'obs': obs, 'time':0})\n",
    "# env.observation_space = gym.spaces.Dict(obs=env.observation_space, time=gym.spaces.Discrete(1))\n",
    "# print(type(env.observation_space))\n",
    "# env = FilterObservation(env, filter_keys=['obs'])\n",
    "\n",
    "x = env.observation_space['observation'].low[0]\n",
    "y = env.action_space.low[0]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize env and set up hyperparameter\n",
    "env = gym.make('FetchReach-v2')\n",
    "# env = TransformObservation(env, lambda obs: {'obs': obs})\n",
    "# env.observation_space = gym.spaces.Dict(obs=env.observation_space, time=gym.spaces.Discrete(1))\n",
    "# print(env.reset())\n",
    "# print(isinstance(env.observation_space,gym.spaces.Dict))\n",
    "# print(env.observation_space)\n",
    "# env = FlattenObservation(FilterObservation(env, filter_keys=['obs']))\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "obs = unpackObs(env.reset())\n",
    "print(obs)\n",
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space['observation'].shape[0]\n",
    "min_action = env.action_space.low[0]\n",
    "max_action = env.action_space.high[0]\n",
    "print(action_size,state_size,min_action,max_action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v2')\n",
    "\n",
    "# set seed\n",
    "seed = 31\n",
    "# env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# create replay buffer\n",
    "replay_size = 1000000 # size of replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=replay_size)    \n",
    "\n",
    "# target update hyperparameters\n",
    "# start_training_after = 50 # start training NN after this many timesteps\n",
    "# update_target_every = 1 # update target network every this steps\n",
    "tau = 0.001\n",
    "episodes = 100   \n",
    "discount = 0.99\n",
    "batch_size = 64\n",
    "exploration_noise = OrnsteinUhlenbeckActionNoise(action_size).sample()\n",
    "hidden_size = 256\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "reward_scale = 0.5\n",
    "max_steps = 200\n",
    "\n",
    "# create DDPG Agent\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, hidden_size=hidden_size, \n",
    "                  actor_lr=actor_lr, critic_lr=critic_lr, discount=discount, min_action=min_action,\n",
    "                  max_action=max_action, exploration_noise=exploration_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:03,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Timestep: 400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0371 Critic Loss: 0.2513 Done: False Achived State: [0.52965116 1.1083567  0.9234384 ] Desired State: [1.48503275 0.62027095 0.47777392]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:37<04:31,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 11 Timestep: 2400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0372 Critic Loss: 0.2513 Done: False Achived State: [0.52313786 1.08532108 0.9230897 ] Desired State: [1.20199259 0.62868416 0.41065049]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [01:08<04:02,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 21 Timestep: 4400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0372 Critic Loss: 0.2514 Done: False Achived State: [0.52083834 1.08237851 0.9172804 ] Desired State: [1.2176948  0.66176681 0.45735498]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [01:39<03:30,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 31 Timestep: 6400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0374 Critic Loss: 0.2512 Done: False Achived State: [0.53741844 1.12493645 0.92658061] Desired State: [1.42353331 0.65274008 0.54410106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [02:11<03:05,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 41 Timestep: 8400 Total reward: -197.0 Episode length: 200.0 Actor Loss: 0.0370 Critic Loss: 0.2515 Done: False Achived State: [0.82785684 0.82131387 0.96165085] Desired State: [1.29127968 0.75266609 0.58258288]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [02:42<02:29,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51 Timestep: 10400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0370 Critic Loss: 0.2514 Done: False Achived State: [0.51154435 1.0583206  0.90936741] Desired State: [1.47921215 0.7133448  0.40530502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [03:14<01:59,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 61 Timestep: 12400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0371 Critic Loss: 0.2513 Done: False Achived State: [0.37921901 0.54045009 0.93409025] Desired State: [1.21255613 0.75608811 0.40204118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [03:45<01:27,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 71 Timestep: 14400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0369 Critic Loss: 0.2510 Done: False Achived State: [0.82949944 0.76846241 0.99930161] Desired State: [1.32386448 0.75156115 0.44836751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [04:16<00:56,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 81 Timestep: 16400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0372 Critic Loss: 0.2511 Done: False Achived State: [0.51228089 1.05608633 0.91368836] Desired State: [1.42911675 0.79200021 0.52333617]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [04:48<00:25,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 91 Timestep: 18400 Total reward: -200.0 Episode length: 200.0 Actor Loss: 0.0372 Critic Loss: 0.2509 Done: False Achived State: [0.37602653 0.54995666 0.91807869] Desired State: [1.45757396 0.84947088 0.40746909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:13<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "stats_rewards_list = [] # store stats for plotting in this\n",
    "# stats_every = 10 # print stats every this many episodes\n",
    "total_reward = 0\n",
    "timesteps = 0\n",
    "episode_length = 0\n",
    "stats_actor_loss, stats_critic_loss = [], []\n",
    "\n",
    "t1 = time.time()\n",
    "for ep in tqdm(range(episodes)):\n",
    "    \n",
    "    achieved_g,desired_g,state,state_prime = unpackObs(env.reset())\n",
    "    action = agent.select_action(state[3:])\n",
    "    # print(state.shape)\n",
    "    # print(state)\n",
    "    # print(state_prime)\n",
    "    \n",
    "\n",
    "    # train in each episode until episode is done\n",
    "    #while True:\n",
    "    for s in range(max_steps):\n",
    "        timesteps += 1\n",
    "        episode_length += 1\n",
    "        #env.render()\n",
    "        # select an action from the agent's policy\n",
    "        #action = agent.select_action(state[3:])\n",
    "        # enter action into the env\n",
    "        observation, reward, done,x ,info = env.step(action)\n",
    "        #print(reward)\n",
    "        total_reward += reward\n",
    "        \n",
    "        achieved_g,desired_g,next_state,next_state_prime = unpackObs_new(observation)\n",
    "        replay_buffer.add((state, next_state, action, reward*reward_scale,1-float(done)))\n",
    "        \n",
    "        # add experience to replay buffer\n",
    "        \n",
    "        \n",
    "        # HER experience\n",
    "#         substitute_g = achieved_g.copy()\n",
    "#         substitute_reward = env.compute_reward(achieved_g, substitute_g, info)\n",
    "#         replay_buffer.add((state_prime, next_state_prime, action, substitute_reward*reward_scale,0))\n",
    "        \n",
    "        \n",
    "        # train the agent\n",
    "        if len(replay_buffer.storage) > batch_size:\n",
    "            actor_loss, critic_loss = agent.train(replay_buffer, batch_size)\n",
    "            stats_actor_loss.append(actor_loss) \n",
    "            stats_critic_loss.append(critic_loss) \n",
    "            agent.update_target_network_soft()\n",
    "\n",
    "        # print()\n",
    "        # state = next_state\n",
    "        state = next_state\n",
    "        action = agent.select_action(next_state[3:])\n",
    "         \n",
    "        if (done) or s==(max_steps-1) : #\n",
    "\n",
    "            \n",
    "            stats_rewards_list.append((ep, total_reward, episode_length))\n",
    "             \n",
    "            if (ep-1)%10==0:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                    'Timestep: {}'.format(timesteps),\n",
    "                    'Total reward: {:.1f}'.format(total_reward),\n",
    "                    'Episode length: {:.1f}'.format(episode_length),\n",
    "                    'Actor Loss: {:.4f}'.format(np.mean(stats_actor_loss)), \n",
    "                    'Critic Loss: {:.4f}'.format(np.mean(stats_critic_loss)),\n",
    "                    'Done: {}'.format(done),\n",
    "                    'Achived State: {}'.format(achieved_g),\n",
    "                    'Desired State: {}'.format(desired_g))\n",
    "                \n",
    "            stats_actor_loss, stats_critic_loss = [], []\n",
    "            total_reward = 0\n",
    "            episode_length = 0 \n",
    "            break\n",
    "        \n",
    "        \n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.0879921913147\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(t2-t1)\n",
    "def get_running_stat(stat, stat_len):\n",
    "    cum_sum = np.cumsum(np.insert(stat, 0, 0)) \n",
    "    return (cum_sum[stat_len:] - cum_sum[:-stat_len]) / stat_len\n",
    "\n",
    "episode, r, l = np.array(stats_rewards_list).T\n",
    "cum_r = get_running_stat(r, 10)\n",
    "cum_l = get_running_stat(l, 10)\n",
    "\n",
    "# plot rewards\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(episode, r, alpha = 0.5, label='original')\n",
    "plt.plot(episode[-len(cum_r):], cum_r, alpha = 0.5, label='average')\n",
    "plt.title('Episode vs Reward')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "i+=1\n",
    "plt.savefig('image'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "stats_rewards_list = [] # store stats for plotting in this\n",
    "# stats_every = 10 # print stats every this many episodes\n",
    "total_reward = 0\n",
    "timesteps = 0\n",
    "episode_length = 0\n",
    "stats_actor_loss, stats_critic_loss = [], []\n",
    "\n",
    "t1 = time.time()\n",
    "for ep in range(episodes):  # tqdm for episode progress\n",
    "\n",
    "    print(ep)\n",
    "    \n",
    "    achieved_g, desired_g, state, state_prime = unpackObs(env.reset())\n",
    "\n",
    "    # train in each episode until episode is done\n",
    "    while True:\n",
    "        timesteps += 1\n",
    "        episode_length += 1\n",
    "        \n",
    "        # select an action from the agent's policy\n",
    "        action = agent.select_action(state[3:])\n",
    "        \n",
    "        # enter action into the env\n",
    "        observation, reward, done, x, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        achieved_g, desired_g, next_state, next_state_prime = unpackObs_new(observation)\n",
    "        \n",
    "        # add experience to replay buffer\n",
    "        replay_buffer.add((state, next_state, action, reward * reward_scale, 1 - float(done)))\n",
    "        \n",
    "        # train the agent\n",
    "        if len(replay_buffer.storage) > batch_size:\n",
    "            actor_loss, critic_loss = agent.train(replay_buffer, batch_size)\n",
    "            stats_actor_loss.append(actor_loss) \n",
    "            stats_critic_loss.append(critic_loss) \n",
    "            agent.update_target_network_soft()\n",
    "            \n",
    "        if done:\n",
    "            stats_rewards_list.append((ep, total_reward, episode_length))\n",
    "            \n",
    "            # Print episode stats\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Timestep: {}'.format(timesteps),\n",
    "                  'Total reward: {:.1f}'.format(total_reward),\n",
    "                  'Episode length: {:.1f}'.format(episode_length),\n",
    "                  'Actor Loss: {:.4f}'.format(np.mean(stats_actor_loss)), \n",
    "                  'Critic Loss: {:.4f}'.format(np.mean(stats_critic_loss)))\n",
    "            \n",
    "            # Reset stats\n",
    "            stats_actor_loss, stats_critic_loss = [], []\n",
    "            total_reward = 0\n",
    "            episode_length = 0 \n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, a, r, d = replay_buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor(x1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
